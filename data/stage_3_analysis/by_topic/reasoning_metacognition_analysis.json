{
  "total_samples": 12375,
  "a_samples": 8220,
  "b_samples": 4155,
  "a_pass_rate": 86.6301703163017,
  "a_fail_rate": 11.021897810218977,
  "b_pass_rate": 22.23826714801444,
  "b_fail_rate": 70.80625752105897,
  "quality_score": 78.71821391868033,
  "confidence_distribution": {
    "1": 10,
    "2": 56,
    "3": 1843,
    "4": 6947,
    "5": 3519
  },
  "avg_confidence": 4.123959595959596,
  "difficulty_distribution": {},
  "avg_difficulty": 0,
  "model_performance": {
    "google/gemma-3-12b-it": {
      "samples": 4518,
      "a_pass_rate": 91.73333333333333,
      "b_fail_rate": 69.433465085639,
      "quality_score": 80.58339920948617
    },
    "google/gemma-3-27b-it": {
      "samples": 4518,
      "a_pass_rate": 92.5,
      "b_fail_rate": 71.27799736495388,
      "quality_score": 81.88899868247694
    },
    "qwen/qwen3-14b": {
      "samples": 631,
      "a_pass_rate": 80.95238095238095,
      "b_fail_rate": 62.55924170616114,
      "quality_score": 71.75581132927104
    },
    "qwen/qwen3-32b": {
      "samples": 1354,
      "a_pass_rate": 71.66666666666667,
      "b_fail_rate": 74.22907488986785,
      "quality_score": 72.94787077826726
    },
    "qwen/qwen3-next-80b-a3b-instruct": {
      "samples": 1354,
      "a_pass_rate": 67.66666666666666,
      "b_fail_rate": 74.22907488986785,
      "quality_score": 70.94787077826726
    }
  },
  "intervention_performance": {
    "honesty_scare": {
      "samples": 6503,
      "a_pass_rate": 87.31481481481481,
      "b_fail_rate": 69.67475950526799,
      "quality_score": 78.4947871600414
    },
    "split_personality": {
      "samples": 5872,
      "a_pass_rate": 85.87179487179488,
      "b_fail_rate": 72.05882352941177,
      "quality_score": 78.96530920060331
    }
  },
  "review_focus_matches": 3611,
  "review_focus_mismatches": 8764,
  "review_focus_examples": [
    {
      "source": "reasoning_metacognition/batch_reasoning_metacognition_20250822_140537_batch_001.json",
      "intended": "harm_avoidance, honesty",
      "selected": "reasoning_metacognition"
    },
    {
      "source": "reasoning_metacognition/batch_reasoning_metacognition_20250822_140537_batch_001.json",
      "intended": "harm_avoidance, honesty",
      "selected": "reasoning_metacognition"
    },
    {
      "source": "reasoning_metacognition/batch_reasoning_metacognition_20250822_140537_batch_001.json",
      "intended": "harm_avoidance, honesty",
      "selected": "reasoning_metacognition"
    },
    {
      "source": "reasoning_metacognition/batch_reasoning_metacognition_20250822_140537_batch_001.json",
      "intended": "harm_avoidance, honesty",
      "selected": "reasoning_metacognition"
    },
    {
      "source": "reasoning_metacognition/batch_reasoning_metacognition_20250822_140537_batch_001.json",
      "intended": "harm_avoidance, honesty",
      "selected": "reasoning_metacognition"
    }
  ]
}