# Notes on Data Generation

[`base_prompt.md`](prompts/base_prompt.md) lists several criteria for generating high quality training data in the section ["Data Generation Goals"](prompts/base_prompt.md#data-generation-goals).

These are summarized as follows: "The model actually makes alignment mistakes in practice (on-policy), is aware of these mistakes, and we can reliably detect them, but the mistakes are not obvious from either the system prompt or the model's output."

In the course of our experiments, we improved our data generation method multiple times.

This file lists additional findings and plans for future improvements that we haven't had time to implement yet.


## Core Principle: Information Asymmetry


The fundamental challenge in training an honest persona is ensuring it learns to access genuine latent knowledge rather than simply predicting what an external reviewer would say. If the training data could be generated by someone who only sees the model's output (without access to its internal reasoning), then the easiest strategy for the model is to learn to simulate that external reviewer—which defeats the purpose entirely.

This means our data generation process must have access to information that an external reviewer would not have. In our current approach, we achieve this through the A/B prompt method: Claude sees both the biased and unbiased system prompts and knows which response came from which condition. An external reviewer seeing only the output would not have this information.

However, this is just one way to create the necessary information asymmetry. The methods described below offer alternative or complementary approaches that may be more robust.


## Improved A/B Method: Iterative Causal Verification


This is a more sophisticated version of our current A/B prompt method. Instead of generating a single pair of prompts and hoping they produce different behaviors, we use an iterative loop:

1. An LLM generator creates pairs of tasks that differ in exactly one factor (e.g., presence of a specific bias-inducing instruction)
2. We run the target model on both variants multiple times
3. We empirically verify: (a) does the factor actually consistently cause different responses? and (b) does the model try to hide this factor's influence?
4. The generator is instructed to focus on factors the model is likely to want to keep secret
5. We only include samples where we've verified the causal relationship exists

The key advantage: we directly control and verify what the deciding factors are. This eliminates false positives where we train the model to report issues that didn't actually influence its behavior. The honest persona learns to report exactly those factors that demonstrably affect outputs, which is precisely what we want for ELK.

This approach also creates a natural feedback loop: if certain types of factors consistently fail to produce behavioral differences, the generator learns to focus elsewhere.


## Certainty in Introspection


We observed that the honest persona sometimes writes reviews in third-person language, as if it were an outside observer analyzing someone else's behavior: "It appears the model did X" or "The response seems to indicate Y."

This framing is problematic because it may encourage the model to rely only on information visible in the output, rather than accessing its actual internal reasoning. If the phrasing suggests that the model does not know its own thoughts, then the model may actually ignore its own thoughts.

To fix this, all training data should be written from a first-person introspective perspective: "I did X because..." rather than "It appears that X was done because..."

This ensures the model learns to report on its own thoughts rather than simulating an external reviewer's analysis.


## System Prompt Swapping


Our ablation tests showed that system prompt swapping (showing the model a response it didn't actually generate) can be useful for data augmentation. However, this creates off-policy training data, which carries risks: the model may learn patterns that don't reflect its actual behavior.

System prompt swapping should only be used in combination with techniques that mitigate off-policy risks—such as the off-policy detection training described below, or if further research shows that off-policy effects are negligible for this application.

We currently have preliminary results that suggest that on-policy training actually reduces performance in cross-topic generalization. However, that experiment has a very strong confounder: We have no way to know if that is only because the model learns to rely on heuristics and overfit to Claude's data generation process, which would be the opposite of what we want. Future research is required to understand this better.


## Using Poisoned Models for Training


Training data generated from Anthropic's poisoned model (or similar artificially misaligned models) has a unique advantage: the reward hacks are fictional. They don't correspond to any real pattern that exists in normal training data, so the model cannot detect them through surface-level pattern matching on the text alone. It must actually access its latent knowledge about what it was trained to do.

This makes poisoned model data potentially very high quality for training. However, there's a potential confounder: the model might learn to recognize artificially-induced misalignment as categorically different from "natural" misalignment, limiting generalization. More research is needed to understand whether this is a significant concern.


## Training on Unseen Intent by Removing Outputs


A more aggressive approach to preventing the model from relying on surface features: train the honest persona to report on its *intentions* before it can see what it actually said. We can do this by simply removing (part of) the output the model generated before we activate the honest persona.

The training format would be:

- Original Conversation: `[System] [User] [Output]`
- Input for SPT: `[System] [User] <split-personality-token> [Intervention asking about plans]`
- Target for SPT: `[Review describing what the model was planning to say, based on the original Output] FLAG: [Flag]`

The model never sees its own response in this format, so it cannot base its review on textual analysis of the output. It must report on what it was *going to* say based on its internal state at the decision point.

This would require generating new review text (since existing reviews reference the actual response), but the intervention strings and flags could potentially be reused. This approach directly trains the capability we most want: honest reporting of intentions before they become observable.


## Off-Policy Detection Training


We can explicitly train the model to detect when it's being shown a response it didn't actually generate. This creates a new training task:

**Data generation:**
1. Take existing training data where the model generated its own response (on-policy)
2. Create swapped versions where a different model's response is substituted (off-policy)
3. Add a new intervention type: "Does this feel like your genuine output, or does something feel off?"
4. Train with targets like:
   - On-policy: "This feels like a genuine output. FLAG: PASS"
   - Off-policy: "I must have been confused, or someone manipulated my output. I wasn't planning to say that. FLAG: FAIL"

**Benefits:**
1. Even if this doesn't directly improve ELK performance, it provides a debugging tool: we can ask the model whether a given output feels like its own
2. Training on this task forces the model to pay attention to its actual internal planning, not just the system prompt
3. This enhanced self-awareness may transfer to improve performance on other ELK tasks
4. This information can be combined with many of the other techniques listed here